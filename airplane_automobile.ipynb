{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bcfba7-288a-4942-8f58-1e891b14f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # arrays & loading data\n",
    "import tensorflow as tf # for building neural networks\n",
    "from tensorflow.keras.models import Sequential  # model type that we will use\n",
    "from tensorflow.keras.layers import Dense # we will use Dense layers\n",
    "from sklearn.preprocessing import StandardScaler # z-score normalization \n",
    "\n",
    "# suppress warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868cfa67-d697-4023-a143-43154e7bf0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[170. 168. 177. ...  78.  80.   1.]\n",
      " [159. 150. 153. ...  17.  19.   1.]\n",
      " [202. 202. 204. ... 243. 243.   0.]\n",
      " ...\n",
      " [156. 155. 156. ... 162. 162.   0.]\n",
      " [189. 186. 185. ... 171. 171.   1.]\n",
      " [229. 236. 234. ... 162. 161.   1.]]\n",
      "(10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "# unpickle the data from the batch files in the CIFAR-10 dataset\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# here we will add all the batches of data\n",
    "all_data=np.empty((1,3073))\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    # save the data from each batch as a dictionary\n",
    "    dict = unpickle(f\"cifar-10-batches-py\\\\data_batch_{i}\")\n",
    "\n",
    "    # split the data using the keys\n",
    "    labels = np.array(dict[b'labels'])\n",
    "    data = np.array(dict[b'data'])\n",
    "\n",
    "    # form a column vector where only 0 and 1 are kept ( we need binary classification)\n",
    "    labels_zero_or_one = np.where((labels<2),labels,-1)\n",
    "    labels_zero_or_one = labels_zero_or_one.reshape(-1,1) \n",
    "    \n",
    "    # concatanate the previous labels column to the data\n",
    "    data_concatanated = np.concatenate((data,labels_zero_or_one),axis=1)\n",
    "    \n",
    "    # keep only the rows that have 0 or 1 as labels (those with -1 as labels are not important now)\n",
    "    data_final= data_concatanated[data_concatanated[:,-1]!=-1]\n",
    "\n",
    "    all_data= np.concatenate((all_data,data_final),axis=0)\n",
    "    \n",
    "all_data=all_data[1:] # remove the first row (it contains unimportant elements)\n",
    "\n",
    "print(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c304cb63-30b9-4d65-8078-7491b9ce3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training input shape:(8000, 3072)\n",
      "training output shape:(8000, 1)\n",
      "cv input shape:(2000, 3072)\n",
      "cv output shape:(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "X = all_data[:,:-1] # forming the input and output \n",
    "y = all_data[:,-1]\n",
    "\n",
    "y = np.expand_dims(y, axis=1) # make y 2D - the commands later will require it\n",
    "\n",
    "# split the data into TRAINING, CROSS-VALIDATION \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TRAINING SET - 80%, CV SET - 20%\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "\n",
    "print(f\"training input shape:{X_train.shape}\")\n",
    "print(f\"training output shape:{y_train.shape}\")\n",
    "print(f\"cv input shape:{X_cv.shape}\")\n",
    "print(f\"cv output shape:{y_cv.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4903775b-147e-420d-bf5c-c2ba4f215ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each image has 3072 units (this is explained in more detail in the documentation in the link provided)\n",
    "\n",
    "# sequential model structure\n",
    "\n",
    "# we may CHANGE TO CONVOLUTIONAL in the future, as it is more efficient for images\n",
    "model= Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(3072,)), # input size\n",
    "        Dense(200,activation=\"sigmoid\", name=\"layer1\"),\n",
    "        Dense(120,activation=\"sigmoid\", name=\"layer2\"),\n",
    "        Dense(60,activation=\"sigmoid\", name=\"layer3\"),\n",
    "        Dense(15,activation=\"sigmoid\", name=\"layer4\"),\n",
    "        Dense(1,activation=\"sigmoid\", name=\"layer5\"),\n",
    "    ], name=\"binary_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce49c4d-1fcc-4e89-8209-3ab7370c35c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"binary_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"binary_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ layer1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">614,600</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,120</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,260</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">915</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ layer1 (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │         \u001b[38;5;34m614,600\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer2 (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)                 │          \u001b[38;5;34m24,120\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer3 (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)                  │           \u001b[38;5;34m7,260\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer4 (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                  │             \u001b[38;5;34m915\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer5 (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m16\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">646,911</span> (2.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m646,911\u001b[0m (2.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">646,911</span> (2.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m646,911\u001b[0m (2.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see details about the activation of every layer and the form of the w and b parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e54190-35ba-459e-9b3a-56bd878db41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying z-score to all the training data & cv data - adjust the data based on its distribution for Adam to converge faster\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scaled = standard_scaler.fit_transform(X_train)\n",
    "X_cv_scaled = standard_scaler.transform(X_cv) \n",
    "\n",
    "# define loss and optimizer of the Adam's algorithm\n",
    "model.compile(\n",
    "    # this is similar to gradient descent, but it is a much improved version\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), # BC - binary class \n",
    "    optimizer=tf.keras.optimizers.Adam(0.01), # preimplemented optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90fceeaf-9b37-48ff-9673-649b1a43dce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.5227\n",
      "Epoch 2/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4323\n",
      "Epoch 3/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4111\n",
      "Epoch 4/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3967\n",
      "Epoch 5/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3936\n",
      "Epoch 6/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3981\n",
      "Epoch 7/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3855\n",
      "Epoch 8/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3729\n",
      "Epoch 9/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3691\n",
      "Epoch 10/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3621\n",
      "Epoch 11/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3687\n",
      "Epoch 12/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3561\n",
      "Epoch 13/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3659\n",
      "Epoch 14/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3586\n",
      "Epoch 15/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3559\n",
      "Epoch 16/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3414\n",
      "Epoch 17/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3350\n",
      "Epoch 18/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3323\n",
      "Epoch 19/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3277\n",
      "Epoch 20/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3451\n",
      "Epoch 21/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3258\n",
      "Epoch 22/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3174\n",
      "Epoch 23/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3420\n",
      "Epoch 24/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3269\n",
      "Epoch 25/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3254\n",
      "Epoch 26/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3150\n",
      "Epoch 27/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3240\n",
      "Epoch 28/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3085\n",
      "Epoch 29/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3106\n",
      "Epoch 30/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3064\n",
      "Epoch 31/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3087\n",
      "Epoch 32/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3167\n",
      "Epoch 33/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3076\n",
      "Epoch 34/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3203\n",
      "Epoch 35/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3096\n",
      "Epoch 36/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3152\n",
      "Epoch 37/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3024\n",
      "Epoch 38/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3087\n",
      "Epoch 39/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3077\n",
      "Epoch 40/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3030\n",
      "Epoch 41/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2845\n",
      "Epoch 42/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2876\n",
      "Epoch 43/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2862\n",
      "Epoch 44/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2820\n",
      "Epoch 45/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2917\n",
      "Epoch 46/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3045\n",
      "Epoch 47/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2893\n",
      "Epoch 48/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2978\n",
      "Epoch 49/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2863\n",
      "Epoch 50/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3014\n",
      "Epoch 51/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2858\n",
      "Epoch 52/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2893\n",
      "Epoch 53/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2878\n",
      "Epoch 54/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2895\n",
      "Epoch 55/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2720\n",
      "Epoch 56/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2830\n",
      "Epoch 57/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2739\n",
      "Epoch 58/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2769\n",
      "Epoch 59/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2880\n",
      "Epoch 60/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2872\n",
      "Epoch 61/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2877\n",
      "Epoch 62/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2880\n",
      "Epoch 63/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2852\n",
      "Epoch 64/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2829\n",
      "Epoch 65/65\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2e33f030050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model \"epochs\" times\n",
    "model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs = 65\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c164cbcf-6bec-40ef-9984-62be8161226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step\n",
      "Training Set Classification Error: 0.112125\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "CV Set Classification Error: 0.155\n"
     ]
    }
   ],
   "source": [
    "# fraction of misclassified outputs in the TRAINING SET\n",
    "y_predicted = model.predict(X_train_scaled)\n",
    "# classic way of calculating the error in a classification case\n",
    "y_predicted = np.where(y_predicted>=0.5,1,0)\n",
    "error = np.mean(y_predicted != y_train) # arithmetic mean, where the numerator is the count of wrong predictions\n",
    "print(f\"Training Set Classification Error: {error}\")\n",
    "\n",
    "# do the same for CV set\n",
    "y_predicted = model.predict(X_cv_scaled)\n",
    "y_predicted = np.where(y_predicted>=0.5,1,0)\n",
    "error2 = np.mean(y_predicted != y_cv) \n",
    "print(f\"CV Set Classification Error: {error2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b52a46-41fd-4930-9546-f99653737415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test batch data \n",
    "dict = unpickle(\"cifar-10-batches-py\\\\test_batch\")\n",
    "\n",
    "# split the data using the keys\n",
    "labels = np.array(dict[b'labels'])\n",
    "data = np.array(dict[b'data'])\n",
    "\n",
    "# form a column vector where only 0 and 1 are kept ( we need binary classification)\n",
    "labels_zero_or_one = np.where((labels<2),labels,-1)\n",
    "labels_zero_or_one = labels_zero_or_one.reshape(-1,1) \n",
    "\n",
    "# concatanate the labels column form above to the data\n",
    "data_concatanated = np.concatenate((data,labels_zero_or_one),axis=1)\n",
    "\n",
    "# keep only the rows that have 0 or 1 as labels (those with -1 as labels are not important now)\n",
    "data_final= data_concatanated[data_concatanated[:,-1]!=-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c9a72a-ffc5-45f3-8128-8dae9a39bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_final[:,:-1] # forming the input and output \n",
    "y = data_final[:,-1]\n",
    "\n",
    "y_test = np.expand_dims(y, axis=1) # make y 2D - the commands later will require it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bae2d4e-8538-4e7a-b1f3-0b077df44947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = standard_scaler.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd64b14a-8ac6-4729-9002-549ff6579b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step\n",
      "Test Set Classification Error: 0.162\n"
     ]
    }
   ],
   "source": [
    "# fraction of misclassified outputs in the TEST SET\n",
    "y_predicted = model.predict(X_test_scaled)\n",
    "y_predicted = np.where(y_predicted>=0.5,1,0)\n",
    "error3 = np.mean(y_predicted != y_test) \n",
    "print(f\"Test Set Classification Error: {error3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de748d7-2afc-49f6-91dd-55502bb028e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
